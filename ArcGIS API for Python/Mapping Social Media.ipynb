{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Social Media\n",
    "\n",
    "### Introduction\n",
    "Most of us are on social media be it Facebook, Twitter, Instagram or WhatsApp. We all celebrate different events that also take place in our day to day lives, be it a birthday, an internationally recognised day or even political posts appear on social media. \n",
    "\n",
    "In this tutorial we are not going to do anything politically related. Instead, we are going to use the power of **Python** to find out *where* people were taking part in the just ended *#GlobalSurveyorsDay* celebrations.  \n",
    "\n",
    "So I have personally selected one social media platform which I know is where we can actually get to find most of the data that we are looking for only for this exercise. \n",
    "\n",
    "### What Do You Need? \n",
    "\n",
    "In order to be able to follow along through this step-by-step guide, you are going to need the following: \n",
    "\n",
    "- Twitter Developer Account \n",
    "\n",
    "We are going to need this account to be able to use the *Twitter API* which we will integrate with our Python code to give us what we need. \n",
    "\n",
    "- An ArcGIS Organizational Account \n",
    "\n",
    "Since this is all about *Where*, we defintely need that Geographical Information Systems platform and what better place to represent this than in an Online environment where anybody can view and leave a comment. \n",
    "\n",
    "- Jupyter Notebooks (or any code editor) \n",
    "\n",
    "Of course you definitely need a place to write your code and execute. I prefer these Jupyter Notebook but if you have ArcGIS Pro, it comes embedded with these too so you can just open them in there. \n",
    "Don't like notebooks? Well then I guess you can use code editors like Visual Studio Code for this exercise then. \n",
    "\n",
    "## Let's Get to Work\n",
    "\n",
    "So I also left a few things above because those do not have anything to do with our programming which we are about to indulge into. \n",
    "\n",
    "We are going to need the following Python libraries to execute our task well enough: \n",
    "\n",
    "- tweepy\n",
    "\n",
    "An easy-to-use Python library for accessing the Twitter API.\n",
    "\n",
    "- dotenv\n",
    "\n",
    "For keeping our secret credential safe in a place where you can't see them. \n",
    "\n",
    "- pandas\n",
    "\n",
    "For handling our data in a manner that we want. \n",
    "\n",
    "- re\n",
    "\n",
    "For using Regular expressions for some conversions and detection. \n",
    "\n",
    "## Installation of Packages\n",
    "\n",
    "Just like any other foreign packages, these need to be installed and we are just going to do that here. \n",
    "\n",
    "### 1. Installing Tweepy\n",
    "\n",
    "Its simple. Just run the command below in the terminal. \n",
    "\n",
    "`pip install tweepy` \n",
    "\n",
    "### 2. Installing dotenv\n",
    "\n",
    "Well I am not going to steal the show on this one because I actually had to google in order for me to see how its done. You can read the official post from [Medium](https://yuthakarn.medium.com/how-to-not-show-credential-in-jupyter-notebook-c349f9278466) which explains how its done. \n",
    "\n",
    "They are several methods to do so but I prefered using the **python-dotenv** methods where you just run: \n",
    "\n",
    "`pip install python-dotenv` \n",
    "\n",
    "So in the end my `.env` file looked like this: \n",
    "\n",
    "```.env\n",
    "twitter_consumer_key=<my_consumer_key>\n",
    "twitter_consumer_secret=<my_consumer_secret>\n",
    "twitter_access_key=<my_access_key>\n",
    "twitter_access_secret=<my_access_secret>\n",
    "```\n",
    "\n",
    "I know you probably wondering where I got the four values I need to place in that `.env` file from. Remember when I said we need a Twitter Developer account? Yes, above. \n",
    "\n",
    "### Register for a Twitter Dev Account. \n",
    "\n",
    "Okay so here it is: \n",
    "\n",
    "- first you need a normal user twitter account. You can go and [create one](https://twitter.com/) if you did not have any. \n",
    "- then you need to register for a developer account. You can apply on their [developers site](https://developer.twitter.com/en/apply-for-access)\n",
    "- you will need to read the guidelines and accept the terms and conditions too\n",
    "- finally, wait for verification. It normally takes a day or two to get verified and you ae ready to go. \n",
    "- in case they reject your application, you need to read the policies and figure out which one your application violated and re-apply. \n",
    "\n",
    "Assuming you now have that Dev Account, lets get to focus more on this tutorial. \n",
    "\n",
    "## Login \n",
    "\n",
    "As usual, like in any other [previous tutorials](https://github.com/African-Surveyors-Connect/LearnArcGIS/tree/main/ArcGIS%20API%20for%20Python/Publications) that I have released, we need to login to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter password: ········\n"
     ]
    }
   ],
   "source": [
    "from arcgis.gis import GIS\n",
    "import tweepy \n",
    "\n",
    "gis = GIS(\"https://arcgis.com\", \"kumbirai_matingo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries \n",
    "\n",
    "We are going to import our libraries here for usage. \n",
    "\n",
    "- os\n",
    "- dotenv\n",
    "\n",
    "**Do not forget to `load_dotenv`** otherwise those secret keeys will not be detected. \n",
    "\n",
    "After importing, we need to create variables for our secrets so that we can reference them later on by just calling the name of the variable. \n",
    "\n",
    "To call the value from our `.env` file,  we need to call in the `os.getenv()` method and pass in the variable name we assigned in the `.env` file as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Dev Credentials\n",
    "import os\n",
    "from dotenv import load_dotenv # add this line\n",
    "\n",
    "load_dotenv() # add this line\n",
    "\n",
    "consumer_key = os.getenv('twitter_consumer_key') \n",
    "consumer_secret = os.getenv('twitter_consumer_secret')\n",
    "access_key = os.getenv('twitter_access_key')\n",
    "access_secret = os.getenv('twitter_access_secret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, let us create our authentication system. We call in the `OAuthHandler()` method, pass in the *secrets* as parameters and we also do the same with the `set_access_token()` but now we will be using the keys to set the access tokens. \n",
    "\n",
    "Then we tell the **API** that we have authenticated with these parameters can you verify and give me access to Twitter data. \n",
    "\n",
    "If you keys and secrets are correct, you will not see any errors below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)   \n",
    "auth.set_access_token(access_key, access_secret) \n",
    "  \n",
    "# Creating an API object \n",
    "api = tweepy.API(auth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Surveyors Day Tweets\n",
    "\n",
    "The code below might be a bit confusing but here is an explanation. \n",
    "\n",
    "- we created a variable for every expression that we will need later on\n",
    "- tell tweepy to call the `Cursor()` method which helps the **api** to perform a search using the keywords passed in the `q` expression. \n",
    "- we use the extended `tweet_mode` is we need to get all the data associated with a single tweet. \n",
    "- the `pages()` basically handles pagination in case we have soo much tweets we need to extract. For this tutorial I will not be paginating anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search GlobalSurveyorsDay tweets \n",
    "gsd = tweepy.Cursor(api.search, q=\"GlobalSurveyorsDay\", tweet_mode='extended').pages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play around with Loops\n",
    "\n",
    "- we create an empty dictionary in which we are going to store each and every tweet \n",
    "- create a **for** loop which goes through each and every result. \n",
    "- create another loop (this time nested) which will store the paginated tweets. In our case we do not have any paginated tweets. \n",
    "- append each tweet after the other and store this information in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = []\n",
    "for page in gsd:\n",
    "    for tweet in page:\n",
    "        all_tweets.append(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just check to see how many tweets in total do we actually have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the **regular expression** we talked about? Import it here as below\n",
    "\n",
    "- create a function which is going to identify URL's in every `tweet_text` \n",
    "- the `tweet_text` stores the data which the user who sent out the tweet actually said. \n",
    "- so the expression is going to find anything that starts with an *https* within the tweet. \n",
    "- the identified URLs are placed in an HTML anchor tag \n",
    "\n",
    "If you are not familiar with HTMl an anchor tag stores this clickable links like [this one](https://africansurveyors.net) which go somewhere on the web. The HTML notation is as below: \n",
    "\n",
    "```HTML\n",
    "<a href=\"https://somewebsite.com/\">A clickable statement</a>\n",
    "```\n",
    "\n",
    "Its actually a nice way for our viewers to see the original source of the tweets. \n",
    "\n",
    "- lastly we truncuate the length of the tweet to avoid long letters. If the user wants to view more that's what the link will be for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def handle_tweet_text(tweet_text):\n",
    "\n",
    "    for url in re.findall(r'(https?://\\S+)', tweet_text):\n",
    "        tweet_text = tweet_text.replace(url, f'<a href=\"{url}\">{url}</a>')\n",
    "\n",
    "    if len(tweet_text) >= 356:\n",
    "        tweet_text = f'{tweet_text[:350]} . . .'\n",
    "\n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have refined our `tweet_text`, let's create a new list which this time is going to be structured. We are going to be storing only the items we want to represent for our project. \n",
    "\n",
    "We have chosen the following items: \n",
    "\n",
    "- id of the tweet\n",
    "- date when the tweet was created\n",
    "- text\n",
    "- hashtags\n",
    "- mentioned users\n",
    "- url\n",
    "- location (which is basically the reason why we are here) \n",
    "- retweet count (how many people retweeted that particular tweet) \n",
    "- the screen name of the user\n",
    "- their profile image\n",
    "- favourite count (the number of users who liked that tweet) \n",
    "\n",
    "This is just a list of what we want, at the end of this tutorial, I am going to include a JSON which contains everything that you can extract out of twitter. *Just hang on* \n",
    "\n",
    "We call the **for** loop again to get the data that we need and store it in a list. \n",
    "\n",
    "In some cases we use `try` and `except` because its not certain if we are going to find anything and we wouldn't want to run into any errors if nothing exists. So the *except* function helps us in placing a default if nothing is found. \n",
    "\n",
    "The we *append* after each and every loop into our list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_refined = []\n",
    "for tweet in all_tweets:\n",
    "    temp = {}\n",
    "    temp['id'] = tweet['id']\n",
    "    #fetch created time\n",
    "    temp['created_at'] = tweet['created_at']\n",
    "    #fetch text\n",
    "    temp['text'] = handle_tweet_text(tweet['full_text'])\n",
    "    #fetch hashtags for the tweet\n",
    "    try:\n",
    "        hashtags_list = tweet['entities']['hashtags']\n",
    "        hashtags = [hashtag['text'] for hashtag in hashtags_list]\n",
    "        if len(hashtags_list)>1:\n",
    "            temp['hashtags'] = \",\".join(hashtags)\n",
    "        else:\n",
    "            temp['hashtags'] = hashtags_list[0]['text']\n",
    "    except:\n",
    "        temp['hashtags'] = ''\n",
    "    #fetch user mentions\n",
    "    mentions_list = [mention['screen_name'] for mention in tweet['entities']['user_mentions']]\n",
    "    temp['mentions'] = \",\".join(mentions_list)\n",
    "    #fetch url for the tweet\n",
    "    try:\n",
    "        temp['url'] = tweet['entities']['urls'][0]['url']\n",
    "    except:\n",
    "        temp['url'] = ''\n",
    "    #Fetch location (if available) for tweet\n",
    "    try:\n",
    "        temp['location'] = tweet['user']['location']\n",
    "    except:\n",
    "        temp['location'] = None\n",
    "    #fetch number of times tweet has been retweeted\n",
    "    temp['retweet_count'] = tweet['retweet_count']\n",
    "    #who posted the tweet?\n",
    "    temp['screen_name'] = tweet['user']['screen_name']\n",
    "    #The profile picture of the source account\n",
    "    temp['profile_image_url'] = tweet['user']['profile_image_url']\n",
    "    # The number of people who like the tweet\n",
    "    temp['favorite_count'] = tweet['favorite_count']\n",
    "    all_tweets_refined.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a *DataFrame*. \n",
    "\n",
    "Remember **Pandas**? We are going to use it here to create that *DataFrame* and convert it into a *CSV* file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to store the DataFrame in a variable named *tweets_df* and then we call the `DataFrame()` method and pass in our list as a parameter. \n",
    "\n",
    "- included below is how you can convert different date and time zones to suit your needs. \n",
    "\n",
    "In the `tz_convert()` where I placed **None** you can replace that with the timezone that you need. \n",
    "\n",
    "For example: I can use UTC and my code for that line will look like so;\n",
    "\n",
    "```python\n",
    "tweets_df['created_at'] = tweets_df['created_at'].map(lambda t: t.tz_convert('UTC'))\n",
    "```\n",
    "\n",
    "You can check outline for different timezones and their representational parameters. \n",
    "\n",
    "The I use the `head()` function to display the first *5* rows in my dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>url</th>\n",
       "      <th>location</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1376839696991461379</td>\n",
       "      <td>2021-03-30 10:12:18</td>\n",
       "      <td>RT @LPSminneapolis: .@NSPSINC @NCEES @GetKidsi...</td>\n",
       "      <td>NationalSurveyorsWeek,GlobalSurveyorsDay</td>\n",
       "      <td>LPSminneapolis,NSPSINC,NCEES,GetKidsintoSurv</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>GetKidsintoSurv</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/9895885785...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1376768912042299393</td>\n",
       "      <td>2021-03-30 05:31:02</td>\n",
       "      <td>Long days, bad weather, over time, complex pro...</td>\n",
       "      <td>GlobalSurveyorsDay</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/QZabC8Krq6</td>\n",
       "      <td>Knoxville, TN</td>\n",
       "      <td>0</td>\n",
       "      <td>bradabennett</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1406228283...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1376308722788564992</td>\n",
       "      <td>2021-03-28 23:02:24</td>\n",
       "      <td>RT @NCEES: Today is #GlobalSurveyorsDay. Learn...</td>\n",
       "      <td>GlobalSurveyorsDay</td>\n",
       "      <td>NCEES</td>\n",
       "      <td></td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>2</td>\n",
       "      <td>chrstnsnjp</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8990613510...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1375760237941166084</td>\n",
       "      <td>2021-03-27 10:42:55</td>\n",
       "      <td>We have chosen to dwell much on the recent #Gl...</td>\n",
       "      <td>GlobalSurveyorsDay,GlobalSurveyorsDay,Python,A...</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/B3jxhZXcoH</td>\n",
       "      <td>Harare, Zimbabwe</td>\n",
       "      <td>0</td>\n",
       "      <td>surveyor_jr</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1348641760...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1375557398312591363</td>\n",
       "      <td>2021-03-26 21:16:55</td>\n",
       "      <td>Long days, bad weather, over time, complex pro...</td>\n",
       "      <td>GlobalSurveyorsDay,PureSurvyeing</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/kFwWYMXsk4</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>0</td>\n",
       "      <td>lfsanjuanPPM</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/9783411868...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id          created_at  \\\n",
       "0  1376839696991461379 2021-03-30 10:12:18   \n",
       "1  1376768912042299393 2021-03-30 05:31:02   \n",
       "2  1376308722788564992 2021-03-28 23:02:24   \n",
       "3  1375760237941166084 2021-03-27 10:42:55   \n",
       "4  1375557398312591363 2021-03-26 21:16:55   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @LPSminneapolis: .@NSPSINC @NCEES @GetKidsi...   \n",
       "1  Long days, bad weather, over time, complex pro...   \n",
       "2  RT @NCEES: Today is #GlobalSurveyorsDay. Learn...   \n",
       "3  We have chosen to dwell much on the recent #Gl...   \n",
       "4  Long days, bad weather, over time, complex pro...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0           NationalSurveyorsWeek,GlobalSurveyorsDay   \n",
       "1                                 GlobalSurveyorsDay   \n",
       "2                                 GlobalSurveyorsDay   \n",
       "3  GlobalSurveyorsDay,GlobalSurveyorsDay,Python,A...   \n",
       "4                   GlobalSurveyorsDay,PureSurvyeing   \n",
       "\n",
       "                                       mentions                      url  \\\n",
       "0  LPSminneapolis,NSPSINC,NCEES,GetKidsintoSurv                            \n",
       "1                                                https://t.co/QZabC8Krq6   \n",
       "2                                         NCEES                            \n",
       "3                                                https://t.co/B3jxhZXcoH   \n",
       "4                                                https://t.co/kFwWYMXsk4   \n",
       "\n",
       "           location  retweet_count      screen_name  \\\n",
       "0                                2  GetKidsintoSurv   \n",
       "1     Knoxville, TN              0     bradabennett   \n",
       "2       Seattle, WA              2       chrstnsnjp   \n",
       "3  Harare, Zimbabwe              0      surveyor_jr   \n",
       "4       Houston, TX              0     lfsanjuanPPM   \n",
       "\n",
       "                                   profile_image_url  favorite_count  \n",
       "0  http://pbs.twimg.com/profile_images/9895885785...               0  \n",
       "1  http://pbs.twimg.com/profile_images/1406228283...               0  \n",
       "2  http://pbs.twimg.com/profile_images/8990613510...               0  \n",
       "3  http://pbs.twimg.com/profile_images/1348641760...               1  \n",
       "4  http://pbs.twimg.com/profile_images/9783411868...               0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.DataFrame(all_tweets_refined)\n",
    "tweets_df['created_at'] = pd.to_datetime(tweets_df['created_at'])\n",
    "tweets_df['created_at'] = tweets_df['created_at'].map(lambda t: t.tz_convert(None))\n",
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DataFrame to CSV file\n",
    "\n",
    "- call the `to_csv()` function and assign a path where we would like to store our csv and **DO NOT** forget to give it a name. \n",
    "\n",
    "I have a folder named **data** in this projects directory, so that's where I chose to store my csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as a CSV file\n",
    "gsd_tweets = tweets_df.to_csv('data\\global_surveyors_day.csv')\n",
    "gsd_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish to ArcGIS Online\n",
    "\n",
    "I will not get into detail about this step since I have touched on this one a few tutorials back. You can refer [to this publication](https://github.com/African-Surveyors-Connect/LearnArcGIS/blob/main/ArcGIS%20API%20for%20Python/Publications/Publishing%20to%20ArcGIS%20Online%20via%20Notebooks%20(using%20Python%20API).pdf) in case you have forgotten what this process of publication means or if you need explanation on why and what. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"item_container\" style=\"height: auto; overflow: hidden; border: 1px solid #cfcfcf; border-radius: 2px; background: #f6fafa; line-height: 1.21429em; padding: 10px;\">\n",
       "                    <div class=\"item_left\" style=\"width: 210px; float: left;\">\n",
       "                       <a href='https://africageoportal.maps.arcgis.com/home/item.html?id=d539318b0aa740c4a7927e7ec49d07ad' target='_blank'>\n",
       "                        <img src='http://static.arcgis.com/images/desktopapp.png' class=\"itemThumbnail\">\n",
       "                       </a>\n",
       "                    </div>\n",
       "\n",
       "                    <div class=\"item_right\"     style=\"float: none; width: auto; overflow: hidden;\">\n",
       "                        <a href='https://africageoportal.maps.arcgis.com/home/item.html?id=d539318b0aa740c4a7927e7ec49d07ad' target='_blank'><b>global_surveyors_day</b>\n",
       "                        </a>\n",
       "                        <br/><img src='https://africageoportal.maps.arcgis.com/home/js/jsapi/esri/css/images/item_type_icons/layers16.png' style=\"vertical-align:middle;\">CSV by kumbirai_matingo\n",
       "                        <br/>Last Modified: April 01, 2021\n",
       "                        <br/>0 comments, 0 views\n",
       "                    </div>\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<Item title:\"global_surveyors_day\" type:CSV owner:kumbirai_matingo>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surveyors_day_tweets = gis.content.add({}, 'data/global_surveyors_day.csv')\n",
    "surveyors_day_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"item_container\" style=\"height: auto; overflow: hidden; border: 1px solid #cfcfcf; border-radius: 2px; background: #f6fafa; line-height: 1.21429em; padding: 10px;\">\n",
       "                    <div class=\"item_left\" style=\"width: 210px; float: left;\">\n",
       "                       <a href='https://africageoportal.maps.arcgis.com/home/item.html?id=16c22c1659ae4183b89bb9288d0617d3' target='_blank'>\n",
       "                        <img src='http://static.arcgis.com/images/desktopapp.png' class=\"itemThumbnail\">\n",
       "                       </a>\n",
       "                    </div>\n",
       "\n",
       "                    <div class=\"item_right\"     style=\"float: none; width: auto; overflow: hidden;\">\n",
       "                        <a href='https://africageoportal.maps.arcgis.com/home/item.html?id=16c22c1659ae4183b89bb9288d0617d3' target='_blank'><b>global_surveyors_day</b>\n",
       "                        </a>\n",
       "                        <br/><img src='https://africageoportal.maps.arcgis.com/home/js/jsapi/esri/css/images/item_type_icons/table16.png' style=\"vertical-align:middle;\">Table Layer by kumbirai_matingo\n",
       "                        <br/>Last Modified: April 01, 2021\n",
       "                        <br/>0 comments, 0 views\n",
       "                    </div>\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<Item title:\"global_surveyors_day\" type:Table Layer owner:kumbirai_matingo>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# created a hosted layer\n",
    "surveyors_day_tweets.publish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have published our data and it's  ready for mapping, let me deliver my promise to you. \n",
    "\n",
    "## That JSON \n",
    "\n",
    "Earlier on I had mentioed about other things you can extract from twitter. This code below is going to help us see what exactly can we get from twitter. \n",
    "\n",
    "### I am going to extract some information from my twitter account\n",
    "\n",
    "This time we do the same thing like we did with the *search* query above but instead, we change a few things. \n",
    "\n",
    "- we tell the API to get from a `user_timeline` \n",
    "- define a screename (in this case that's my twitter account username) \n",
    "\n",
    "The `.items()` function tells the API how many items we want to return. Leaving it blank will extract all information from my timeline and if I have soo many tweets, you might exhaust and go beyong the API's limit. Its always wise to specify how many items you want. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = tweepy.Cursor(api.user_timeline, screename=\"surveyor_jr\", tweet_mode='extended').items(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list. Just like we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_tweets = []\n",
    "for tweet in my_tweets:\n",
    "    retrieved_tweets.append(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter everything and review the contents of a **single** tweet. \n",
    "\n",
    "Check it out for yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieved_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it for now. That is how you can get to extract data from twitter and use the data for various use cases such as analysis or anything you might think of. \n",
    "\n",
    "You can also watch the live video based on this tutorial straight from our **Youtube** channel. Be sure to like and subscribe. Leave a comment also in case you need more explanation. \n",
    "\n",
    "[![Mapping Social Media](https://img.youtube.com/vi/gD15Uz5hph4/0.jpg)](https://www.youtube.com/watch?v=gD15Uz5hph4)\n",
    "\n",
    "If you didn't face any errors along the way and managed to get the data onto the ArcGIS Organizational Platform then;\n",
    "\n",
    "### CONGRATULATIONS \n",
    "\n",
    "![Congratulations](https://media1.giphy.com/media/3oz9ZE2Oo9zRC/200.gif)\n",
    "\n",
    "\n",
    "You've managed to complete your step in **#DataMining** with GIS \n",
    "\n",
    "For anyone having trouble or fails to understand this tutorial, I am reachable via [LinkedIn](https://www.linkedin.com/in/kumbirai-matingo-6910a617a/). Just send me a direct message and I will be sure to respond to any questions relating to the tutorials that you might have. \n",
    "\n",
    "## About Author \n",
    "\n",
    "![Kumbirai Matingo.jpg](https://pbs.twimg.com/profile_images/1348641760575168515/NalgUV_9_200x200.jpg)\n",
    "\n",
    "- 3rd Year BSc Hons in Surveying & Geomatics\n",
    "- Interested in GIS for Health and Land Administration, Spatial Data Science and Programming \n",
    "- You can download my Resume online too. Just click [here](https://africansurveyors.net/images/Resume%20for%20Kumbirai%20Matingo.pdf)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
